{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WYVlMih95BOD",
        "outputId": "aaf12a8a-fd5b-4ed6-a0a7-3d88b26a18cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.7.1+cpu\n",
            "GPU Available: False\n",
            "Loading MNIST dataset...\n",
            "Training data size: 60000\n",
            "Test data size: 10000\n",
            "Number of classes: 10\n",
            "CNN(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (drop1): Dropout(p=0.25, inplace=False)\n",
            "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (drop2): Dropout(p=0.25, inplace=False)\n",
            "  (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (drop3): Dropout(p=0.25, inplace=False)\n",
            "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
            "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (drop4): Dropout(p=0.5, inplace=False)\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (drop5): Dropout(p=0.5, inplace=False)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# MNIST Handwritten Digits Classification with CNN (PyTorch Version)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "print(\"Loading MNIST dataset...\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(f\"Training data size: {len(train_dataset)}\")\n",
        "print(f\"Test data size: {len(test_dataset)}\")\n",
        "print(f\"Number of classes: {len(train_dataset.classes)}\")\n",
        "\n",
        "# Build the CNN model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        self.drop1 = nn.Dropout(0.25)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "        self.drop2 = nn.Dropout(0.25)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(64, 128, kernel_size=3)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.drop3 = nn.Dropout(0.25)\n",
        "\n",
        "        self.fc1 = nn.Linear(128 * 2 * 2, 512)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "        self.drop4 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.drop5 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.bn1(self.conv1(x)))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool1(x)\n",
        "        x = self.drop1(x)\n",
        "\n",
        "        x = F.relu(self.bn2(self.conv3(x)))\n",
        "        x = F.relu(self.conv4(x))\n",
        "        x = self.pool2(x)\n",
        "        x = self.drop2(x)\n",
        "\n",
        "        x = F.relu(self.bn3(self.conv5(x)))\n",
        "        x = self.drop3(x)\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.bn4(self.fc1(x)))\n",
        "        x = self.drop4(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.drop5(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = CNN().to(device)\n",
        "print(model)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "train_losses, val_losses, train_accs, val_accs = [], [], [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "    train_loss = running_loss / total\n",
        "    train_acc = correct / total\n",
        "    train_losses.append(train_loss)\n",
        "    train_accs.append(train_acc)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss, val_correct, val_total = 0.0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item() * images.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_correct += (predicted == labels).sum().item()\n",
        "            val_total += labels.size(0)\n",
        "    val_loss = val_loss / val_total\n",
        "    val_acc = val_correct / val_total\n",
        "    val_losses.append(val_loss)\n",
        "    val_accs.append(val_acc)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - \"\n",
        "          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f} - \"\n",
        "          f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "# Plot training history\n",
        "def plot_training_history(train_accs, val_accs, train_losses, val_losses):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "    ax1.plot(train_accs, label='Training Accuracy')\n",
        "    ax1.plot(val_accs, label='Validation Accuracy')\n",
        "    ax1.set_title('Model Accuracy')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    ax2.plot(train_losses, label='Training Loss')\n",
        "    ax2.plot(val_losses, label='Validation Loss')\n",
        "    ax2.set_title('Model Loss')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_history(train_accs, val_accs, train_losses, val_losses)\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "test_accuracy = np.mean(all_preds == all_labels)\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds))\n",
        "\n",
        "# Confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=range(10), yticklabels=range(10))\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.show()\n",
        "\n",
        "# Visualize predictions on 5 sample images\n",
        "def visualize_predictions(model, test_dataset, num_samples=5):\n",
        "    model.eval()\n",
        "    indices = np.random.choice(len(test_dataset), num_samples, replace=False)\n",
        "    fig, axes = plt.subplots(1, num_samples, figsize=(15, 3))\n",
        "    for i, idx in enumerate(indices):\n",
        "        image, true_label = test_dataset[idx]\n",
        "        image_input = image.unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(image_input)\n",
        "            pred_label = output.argmax(dim=1).item()\n",
        "            confidence = F.softmax(output, dim=1).max().item()\n",
        "        axes[i].imshow(image.squeeze(), cmap='gray')\n",
        "        axes[i].axis('off')\n",
        "        color = 'green' if true_label == pred_label else 'red'\n",
        "        title = f'True: {true_label}\\nPred: {pred_label}\\nConf: {confidence:.3f}'\n",
        "        axes[i].set_title(title, color=color, fontsize=10)\n",
        "    plt.suptitle('Model Predictions on Sample Images', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"\\nVisualizing predictions on 5 sample images:\")\n",
        "visualize_predictions(model, test_dataset, 5)\n",
        "\n",
        "# Per-digit accuracy\n",
        "def analyze_per_digit_performance(y_true, y_pred):\n",
        "    accuracy_per_digit = []\n",
        "    for digit in range(10):\n",
        "        digit_mask = (y_true == digit)\n",
        "        digit_accuracy = np.mean(y_pred[digit_mask] == digit)\n",
        "        accuracy_per_digit.append(digit_accuracy)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    bars = plt.bar(range(10), accuracy_per_digit, color='skyblue', edgecolor='navy')\n",
        "    plt.xlabel('Digit')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Per-Digit Classification Accuracy')\n",
        "    plt.xticks(range(10))\n",
        "    plt.ylim(0, 1)\n",
        "    plt.grid(axis='y', alpha=0.3)\n",
        "    for i, bar in enumerate(bars):\n",
        "        height = bar.get_height()\n",
        "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
        "                 f'{height:.3f}', ha='center', va='bottom')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return accuracy_per_digit\n",
        "\n",
        "print(\"\\nAnalyzing per-digit performance:\")\n",
        "digit_accuracies = analyze_per_digit_performance(all_labels, all_preds)\n",
        "\n",
        "print(f\"\\nSummary Statistics:\")\n",
        "print(f\"Overall Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Best Digit Accuracy: {max(digit_accuracies):.4f} (Digit {np.argmax(digit_accuracies)})\")\n",
        "print(f\"Worst Digit Accuracy: {min(digit_accuracies):.4f} (Digit {np.argmin(digit_accuracies)})\")\n",
        "print(f\"Average Per-Digit Accuracy: {np.mean(digit_accuracies):.4f}\")\n",
        "\n",
        "# Optional: Save the model\n",
        "print(\"\\nSaving model...\")\n",
        "torch.save(model.state_dict(), 'mnist_cnn_model.pth')\n",
        "print(\"Model saved as 'mnist_cnn_model.pth'\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MNIST CNN Classification Complete!\")\n",
        "print(f\"Final Test Accuracy: {test_accuracy*100:.2f}%\")\n",
        "if test_accuracy > 0.95:\n",
        "    print(\"ðŸŽ‰ Successfully achieved >95% accuracy target!\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
